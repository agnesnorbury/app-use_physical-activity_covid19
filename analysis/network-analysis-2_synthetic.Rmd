---
title: "EMA during COVID-19: Network analysis of daily summary data"
output: html_document
---

```{r setup, include=FALSE}
#knitr options
knitr::opts_chunk$set(echo=TRUE, error=TRUE, warning=FALSE, message=FALSE, fig.align="center")
#knitr::opts_chunk$set(width = 200)

#load packages
packages<-c("dplyr", "tidyr", "reshape2", "imputeTS", 
            "huge", "mlVAR", "qgraph", "parSim",
            "ggplot2", "ggpubr", "ggpmisc", "ggcorrplot", "summarytools"
            )
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only=TRUE)

#function for custom colour scales for our subplots
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

```

## Background

Here, we are interested in investigating the *predictive relationships* between **social media usage** (total time spent), **physical activity** (total steps), and **mood** (mean emotional valence), controlling for other (non-social) smartphone use.

To do this, we will use multilevel vector autoregression (mlVAR) on time-series data represented as Gaussian graphical models (GGMs), following the approach outlined in Epskamp et al. 2018 (https://doi.org/10.1080/00273171.2018.1454823), and implemented in the R package mlVAR (https://cran.r-project.org/web/packages/mlVAR/).

This will allow us to generate three networks:

1. **Temporal network**. A GGM which depicts the lagged associations between measures across successive measurement times. This network shows if a deviation from a person's mean in one variable predicts a deviation from a person's mean in another variable at the next measurement occasion, and therefore gives insight into Granger causality.

2. **Between-subjects network**. A GGM which models the covaraition between participants' means across measures (gives insight into random effects, or individual differences).

3. **Contemporaneous network**. A GGM which depicts associations between measures within the same measurement window (here, each day), after controlling for temporal associations.

For an example of this approach previously applied to real-world data (twice-daily PTSD symptom levels during a period of conflict), see Greene et al., 2018 (https://doi.org/10.1017/S0033291718000351).


```{r load_data, include=FALSE}
#load datafile with per day summary of activity variables
data<-read.csv("../data/synthetic-data-2.csv", strip.white=TRUE)

#make sure data is in correct format
data$date.new<-as.Date(data$date.new)
```

First, let's select the data we want:

```{r select_data, echo=FALSE}
#subset data we want to use for this analysis
dfA<-data %>%
     dplyr::select(user, date.new,
            steps_total, social_usage, nonsocial_usage, valence)
```

Then, define the variables to include in the model:

```{r define_measures}
#define measures for the analysis
vars<-c("steps_total","social_usage", "nonsocial_usage", "valence")   
shortnames<-c("steps", "social app", "nonsocial apps", "valence")   
idvar<-"user"
```

Let's also plot the outcomes against each other, to see if we have obviously different time periods in the data, where statistical properties (e.g. mean) of times series and dynamics between variables might differ. 

The vertical dotted line represents the declaration of a state of national emergency in Spain on the 14th March, 2020.

```{r plot_data, echo=FALSE}
#setup cross-plot colour palette for different variables
palette<-gg_color_hue(7) #palette emulating ggplot default (equally spaced colours)
colours<-c("steps_total" = palette[4],
           "social_usage" = palette[5],
           "nonsocial_usage" = palette[6],
           "valence" = palette[7])

#melt data to long format for ggplot
dfA.melt<-melt(dfA, id.vars=c("user","date.new"), na.rm=FALSE)

#get weekends throughtout period
days<-seq(as.Date(min(dfA.melt$date.new)), as.Date(max(dfA.melt$date.new)), by="days")
days.weekdays<-weekdays(days)
#create new variable recording if that day was in a weekend or not
days.is_weekend<-ifelse(grepl("Saturday", days.weekdays), 1,
                        ifelse(grepl("Sunday", days.weekdays), 1,
                        0))
days.is_weekend<-as.logical(days.is_weekend)
#and extract list of weekends for plotting
wknds<-days[days.is_weekend==TRUE]

#plot steps data
dfA.melt.steps<-dfA.melt %>%
                filter(variable=="steps_total")
p1<-ggplot(dfA.melt.steps, aes(x=date.new, y=value, group=variable, color=variable)) +
     theme_minimal() + theme(panel.grid.major.x = element_blank(), 
                             panel.grid.minor.x = element_blank(),
                             panel.grid.minor.y = element_blank(),
                             panel.grid.major.y = element_line(colour="grey", size=.2)) +
     theme(aspect.ratio=0.2) + #coord_fixed() +
     theme(plot.margin=grid::unit(c(0,8,0,4), "mm")) +
     geom_vline(xintercept=wknds, size=2.2, color="grey", alpha=.3) +
     stat_summary(fun.y=mean, geom="line") +
     stat_summary(fun.data=mean_se, geom="ribbon", aes(fill=variable), alpha=.2, colour = NA) +
     scale_x_date(date_breaks = "2 week", date_labels =  "%d-%b") +
     scale_colour_manual(values=colours, name="physical activity", 
                         labels=c("total steps")) +
     scale_fill_manual(values=colours, name="physical activity", 
                         labels=c("total steps")) +
     theme(legend.position="right") + 
     geom_vline(xintercept=as.numeric(as.Date(c("2020-03-14"))), 
                linetype="dashed", color="black", alpha=.5) +
     labs(x="", y="steps")

#plot app usage data
dfA.melt.soc<-dfA.melt %>%
              filter(variable=="social_usage" | variable=="nonsocial_usage")
dfA.melt.soc$value<-dfA.melt.soc$value/60 #recode in minutes
p2<-ggplot(dfA.melt.soc, aes(x=date.new, y=value, group=variable, color=variable)) +
     theme_minimal() + theme(panel.grid.major.x = element_blank(), 
                             panel.grid.minor.x = element_blank(),
                             panel.grid.minor.y = element_blank(),
                             panel.grid.major.y = element_line(colour="grey", size=.2)) +
     theme(aspect.ratio=0.2) + #coord_fixed() +
     theme(plot.margin=grid::unit(c(0,8,0,4), "mm")) +
     geom_vline(xintercept=wknds, size=2.2, color="grey", alpha=.3) +
     stat_summary(fun.y=mean, geom="line") +
     stat_summary(fun.data=mean_se, geom="ribbon", aes(fill=variable), alpha=.2, colour = NA) +
     scale_x_date(date_breaks = "2 week", date_labels =  "%d-%b") +
     scale_colour_manual(values=colours, name="smartphone use", 
                         labels=c("social app use", "other app use")) +
     scale_fill_manual(values=colours, name="smartphone use", 
                         labels=c("social app use", "other app use")) +
     theme(legend.position="right") + 
     geom_vline(xintercept=as.numeric(as.Date(c("2020-03-14"))), 
                linetype="dashed", color="black", alpha=.5) +
     labs(x="", y="time (mins)")

#plot valence data
dfA.melt.val<-dfA.melt %>%
              filter(variable=="valence")
p3<-ggplot(dfA.melt.val, aes(x=date.new, y=value, group=variable, color=variable)) +
     theme_minimal() + theme(panel.grid.major.x = element_blank(), 
                             panel.grid.minor.x = element_blank(),
                             panel.grid.minor.y = element_blank(),
                             panel.grid.major.y = element_line(colour="grey", size=.2)) +
     theme(aspect.ratio=0.2) + #coord_fixed() +
     theme(plot.margin=grid::unit(c(0,8,0,4), "mm")) +
     geom_vline(xintercept=wknds, size=2.2, color="grey", alpha=.3) +
     stat_summary(fun.y=mean, geom="line") +
     stat_summary(fun.data=mean_se, geom="ribbon", aes(fill=variable), alpha=.2, colour = NA) +
     scale_x_date(date_breaks = "2 week", date_labels =  "%d-%b") +
     scale_colour_manual(values=colours, name="EMA data", 
                         labels=c("emotional state")) +
     scale_fill_manual(values=colours, name="EMA data", 
                         labels=c("emotional state")) +
     theme(legend.position="right") + 
     geom_vline(xintercept=as.numeric(as.Date(c("2020-03-14"))), 
                linetype="dashed", color="black", alpha=.5) +
     labs(x="", y="mean valence")

ggarrange(p1, p2, p3, ncol=1, align="v")

```

### With data adjusted for weekly seasonality

For each measure, select data for each user, convert to time series format with weekly periodicity, decompose time series into different components (including seasonal variation, using stl), and then subtract the seasonal component from the time series. 

NB, the decomposition process doesn't tolerate NAs, so here we will impute missing data for each user (using imputeTS), run the adjustment, then remove the imputed values at the end.

```{r adjust_weekend}
users<-as.numeric(unique(dfA$user))
dfA.adj<-dfA

#for steps:
for (u in users) {
  #select daily data for that user only
  dfA.u<-dfA %>%
         filter(user==u) %>%
         arrange(date.new)
  #impute NAs using TS impute as otherwise time series decomposition won't work
  dfA.u$steps_total<-na_interpolation(dfA.u$steps_total)
  #convert to time series data based on start date, with weekly periodicity
  dfA.u.ts<-ts(dfA.u$steps_total, start=as.Date(dfA.u$date.new[1]), frequency=7)
  #identify seasonal components in ts data:
  ts.components<-stl(dfA.u.ts, s.window=7, s.degree = 0) 
  #remove seasonal component from ts
  dfA.u.ts.adj<-(dfA.u.ts - ts.components[["time.series"]][,1]) 
  #add back to df
  dfA.adj$steps_total[dfA.adj$user==u]<-dfA.u.ts.adj
}
#replace imputed data with NAs again
dfA.adj$steps_total[is.na(dfA$steps_total)]<-NA


#for social:
for (u in users) {
  #select daily data for that user only
  dfA.u<-dfA %>%
         filter(user==u) %>%
         arrange(date.new)
  #impute NAs using TS impute as otherwise time series decomposition won't work
  dfA.u$social_usage<-na_interpolation(dfA.u$social_usage)
  #convert to time series data based on start date, with weekly periodicity
  dfA.u.ts<-ts(dfA.u$social_usage, start=as.Date(dfA.u$date.new[1]), frequency=7)
  #identify seasonal components in ts data:
  ts.components<-stl(dfA.u.ts, s.window=7, s.degree = 0) 
  #remove seasonal component from ts
  dfA.u.ts.adj<-(dfA.u.ts - ts.components[["time.series"]][,1]) 
  #add back to df
  dfA.adj$social_usage[dfA.adj$user==u]<-dfA.u.ts.adj
}
#replace imputed data with NAs again
dfA.adj$social_usage[is.na(dfA$social_usage)]<-NA

#for nonsocial:
for (u in users) {
  #select daily data for that user only
  dfA.u<-dfA %>%
         filter(user==u) %>%
         arrange(date.new)
  #impute NAs using TS impute as otherwise time series decomposition won't work
  dfA.u$nonsocial_usage<-na_interpolation(dfA.u$nonsocial_usage)
  #convert to time series data based on start date, with weekly periodicity
  dfA.u.ts<-ts(dfA.u$nonsocial_usage, start=as.Date(dfA.u$date.new[1]), frequency=7)
  #identify seasonal components in ts data [method 2]:
  ts.components<-stl(dfA.u.ts, s.window=7, s.degree = 0) 
  #remove seasonal component from ts
  dfA.u.ts.adj<-(dfA.u.ts - ts.components[["time.series"]][,1]) 
  #add back to df
  dfA.adj$nonsocial_usage[dfA.adj$user==u]<-dfA.u.ts.adj
}
#replace imputed data with NAs again
dfA.adj$nonsocial_usage[is.na(dfA$nonsocial_usage)]<-NA
```

Let's re-plot the data to see how successful this was:

```{r plot_data_adj, echo=FALSE}
#setup cross-plot colour palette for different variables
palette<-gg_color_hue(7) #palette emulating ggplot default (equally spaced colours)
colours<-c("steps_total" = palette[4],
           "social_usage" = palette[5],
           "nonsocial_usage" = palette[6],
           "valence" = palette[7])

#melt data to long format for ggplot
dfA.adj$date.new<-as.Date(dfA.adj$date.new)
dfA.adj.melt<-melt(dfA.adj, id.vars=c("user","date.new"), na.rm=FALSE)

#get weekends throughtout period
days<-seq(as.Date(min(dfA.adj.melt$date.new)), as.Date(max(dfA.adj.melt$date.new)), by="days")
days.weekdays<-weekdays(days)
#create new variable recording if that day was in a weekend or not
days.is_weekend<-ifelse(grepl("Saturday", days.weekdays), 1,
                        ifelse(grepl("Sunday", days.weekdays), 1,
                        0))
days.is_weekend<-as.logical(days.is_weekend)
#and extract list of weekends for plotting
wknds<-days[days.is_weekend==TRUE]

#plot steps data
dfA.melt.steps<-dfA.adj.melt %>%
                filter(variable=="steps_total")
p1<-ggplot(dfA.melt.steps, aes(x=date.new, y=value, group=variable, color=variable)) +
     theme_minimal() + theme(panel.grid.major.x = element_blank(), 
                             panel.grid.minor.x = element_blank(),
                             panel.grid.minor.y = element_blank(),
                             panel.grid.major.y = element_line(colour="grey", size=.2)) +
     theme(aspect.ratio=0.2) + #coord_fixed() +
     theme(plot.margin=grid::unit(c(0,8,0,4), "mm")) +
     geom_vline(xintercept=wknds, size=2.2, color="grey", alpha=.3) +
     stat_summary(fun.y=mean, geom="line") +
     stat_summary(fun.data=mean_se, geom="ribbon", aes(fill=variable), alpha=.2, colour = NA) +
     scale_x_date(date_breaks = "2 week", date_labels =  "%d-%b") +
     scale_colour_manual(values=colours, name="physical activity", 
                         labels=c("total steps")) +
     scale_fill_manual(values=colours, name="physical activity", 
                         labels=c("total steps")) +
     theme(legend.position="right") + 
     geom_vline(xintercept=as.numeric(as.Date(c("2020-03-14"))), 
                linetype="dashed", color="black", alpha=.5) +
     labs(x="", y="steps")

#plot social data
dfA.melt.soc<-dfA.adj.melt %>%
              filter(variable=="social_usage" | variable=="nonsocial_usage")
dfA.melt.soc$value<-dfA.melt.soc$value/60 #recode in minutes
p2<-ggplot(dfA.melt.soc, aes(x=date.new, y=value, group=variable, color=variable)) +
     theme_minimal() + theme(panel.grid.major.x = element_blank(), 
                             panel.grid.minor.x = element_blank(),
                             panel.grid.minor.y = element_blank(),
                             panel.grid.major.y = element_line(colour="grey", size=.2)) +
     theme(aspect.ratio=0.2) + #coord_fixed() +
     theme(plot.margin=grid::unit(c(0,8,0,4), "mm")) +
     geom_vline(xintercept=wknds, size=2.2, color="grey", alpha=.3) +
     stat_summary(fun.y=mean, geom="line") +
     stat_summary(fun.data=mean_se, geom="ribbon", aes(fill=variable), alpha=.2, colour = NA) +
     scale_x_date(date_breaks = "2 week", date_labels =  "%d-%b") +
     scale_colour_manual(values=colours, name="smartphone use", 
                         labels=c("other app use", "social app use")) +
     scale_fill_manual(values=colours, name="smartphone use", 
                         labels=c("other app use", "social app use")) +
     theme(legend.position="right") + 
     geom_vline(xintercept=as.numeric(as.Date(c("2020-03-14"))), 
                linetype="dashed", color="black", alpha=.5) +
     labs(x="", y="time (mins)")

#plot valence data
dfA.melt.val<-dfA.adj.melt %>%
                    filter(variable=="valence")
p3<-ggplot(dfA.melt.val, aes(x=date.new, y=value, group=variable, color=variable)) +
     theme_minimal() + theme(panel.grid.major.x = element_blank(),
                             panel.grid.minor.x = element_blank(),
                             panel.grid.minor.y = element_blank(),
                             panel.grid.major.y = element_line(colour="grey", size=.2)) +
     theme(aspect.ratio=0.2) + #coord_fixed() +
     theme(plot.margin=grid::unit(c(0,8,0,4), "mm")) +
     geom_vline(xintercept=wknds, size=2.2, color="grey", alpha=.3) +
     stat_summary(fun.y=mean, geom="line") +
     stat_summary(fun.data=mean_se, geom="ribbon", aes(fill=variable), alpha=.2, colour = NA) +
     scale_x_date(date_breaks = "2 week", date_labels =  "%d-%b") +
     scale_colour_manual(values=colours, name="EMA data",
                         labels=c("emotional state")) +
     scale_fill_manual(values=colours, name="EMA data",
                         labels=c("emotional state")) +
     theme(legend.position="right") +
     geom_vline(xintercept=as.numeric(as.Date(c("2020-03-14"))),
                linetype="dashed", color="black", alpha=.5) +
     labs(x="", y="mean valence")

ggarrange(p1, p2, p3, ncol=1, align="v")
```

```{r transform_adj, echo=FALSE}
#view original, adjusted, and transformed data
dfA_descr<-dfA %>%
            dplyr::select(-c("user", "date.new"))
view(dfSummary(dfA_descr, graph.magnif=0.75, varnumbers = FALSE, labels.col = FALSE, valid.col = FALSE,
               na.col=FALSE), method="render")

dfA_descr2<-dfA.adj %>%
            dplyr::select(-c("user", "date.new"))
view(dfSummary(dfA_descr2, graph.magnif=0.75, varnumbers = FALSE, labels.col = FALSE, valid.col = FALSE,
               na.col=FALSE), method="render")
```

Next, let's divide the data into pre and post declaration of the national emergency. Let's also transform the data using a nonparanormal transformation (npn), as it appears significantly skewed.

```{r select_data_prepost_adj}
#create new variable dividing dates into relevant time bins:
dfA.adj$date_bin<-ifelse(dfA.adj$date.new < "2020-03-14", 1,
                        ifelse(dfA.adj$date.new >= "2020-03-14", 2,
                      NA))

#subset data we want to use for each analysis
#pre-national emergency declaration:
dfB_pre<-dfA.adj %>%
         filter(date_bin==1)
#post-national emergency declaration:
dfB_post<-dfA.adj %>%
         filter(date_bin==2)

#npn transform, by session
dfB_pre.complete<-dfB_pre[complete.cases(dfB_pre),]
dfB_pre.npn<-dfB_pre.complete
dfB_pre.npn[3:6]<-huge.npn(dfB_pre.npn[3:6])

dfB_post.complete<-dfB_post[complete.cases(dfB_post),]
dfB_post.npn<-dfB_post.complete
dfB_post.npn[3:6]<-huge.npn(dfB_post.npn[3:6])

#melt/cast to get full date coverage
#since mlVAR doesn't make use of the dayvar (date.new) if only one daily observation, we need to add ordering / NA padding to ensure observations line up properly (an no false consecutive days when missing data)
dfB_pre.melt<-melt(dfB_pre.npn, id.vars=c("user","date.new"), na.rm=FALSE) #melt down
dfB_pre.cast<-dcast(dfB_pre.melt, user ~ date.new + variable, median, fill=NaN) #make wide so that all subs have a column for all date/var pairs
dfB_pre.melt2<-melt(dfB_pre.cast, id.vars="user") #melt down to long again, now with missing values filled as NaNs
dfB_pre.melt2<-separate(dfB_pre.melt2, variable, c("date.new","variable"), sep = "_", extra="merge") #split paired date/var back up again
dfB_pre.melt2$value[is.nan(dfB_pre.melt2$value)]<-NA  #replace NaN padding with NAs
dfB_pre.cast2<-dcast(dfB_pre.melt2, user + date.new ~ variable) #create separate columns for each measurement variable again as preferred by mlVAR

#since mlVAR doesn't make use of the dayvar (date.new) if only one daily observation, we need to add ordering / NA padding to ensure observations line up properly (an no false consecutive days when missing data)
dfB_post.melt<-melt(dfB_post.npn, id.vars=c("user","date.new"), na.rm=FALSE) #melt down
dfB_post.cast<-dcast(dfB_post.melt, user ~ date.new + variable, median, fill=NaN) #make wide so that all subs have a column for all date/var pairs
dfB_post.melt2<-melt(dfB_post.cast, id.vars="user") #melt down to long again, now with missing values filled as NaNs
dfB_post.melt2<-separate(dfB_post.melt2, variable, c("date.new","variable"), sep = "_", extra="merge") #split paired date/var back up again
dfB_post.melt2$value[is.nan(dfB_post.melt2$value)]<-NA  #replace NaN padding with NAs
dfB_post.cast2<-dcast(dfB_post.melt2, user + date.new ~ variable) #create separate columns for each measurement variable again as preferred by mlVAR
```

Finally, let's estimate the networks using mlVAR. We'll run this a couple of times (passing backwards then forwards again), to ensure only users who have enough data to contribute to both networks are included in either.

```{r mlVAR_prepost_adj, results='hide'}
#estimate using two-step mlVAR, for post emergency data:
res_mlVAR_post<-mlVAR(dfB_post.cast2, vars, idvar, lags = c(1), temporal="correlated", estimator="lmer")
#restrict users in pre-emergency dataset to only those with enough post emergency data:
subs_post<-as.integer(res_mlVAR_post[["IDs"]])
dfB_pre<-dfB_pre.cast2 %>%
         filter(user %in% subs_post)
#and pass back to exlcude ppts only in post-emergency
res_mlVAR_pre<-mlVAR(dfB_pre.cast2, vars, idvar, lags = c(1), temporal="correlated", estimator="lmer")
subs_pre<-as.integer(res_mlVAR_pre[["IDs"]])
dfB_post<-dfB_post.cast2 %>%
          filter(user %in% subs_pre)

#re-estimate both time periods using two-step mlVAR:
res_mlVAR_pre<-mlVAR(dfB_pre, vars, idvar, lags = c(1), temporal="correlated", estimator="lmer")
res_mlVAR_post<-mlVAR(dfB_post, vars, idvar, lags = c(1), temporal="correlated", estimator="lmer")
```



```{r mlVAR_prepost_results_adj, echo=FALSE}
#number of users in models
print(paste("Number of users included in the pre-emergency model: ", length(res_mlVAR_pre[["IDs"]])),
      quote=FALSE)
print(paste("Number of users included in the post-emergency model: ", length(res_mlVAR_post[["IDs"]])),
      quote=FALSE)
#number of days in models
print(paste("Number of days included in the pre-emergency model: ", length(unique(res_mlVAR_pre[["data"]][["BEEP"]]))),
      quote=FALSE)
print(paste("Number of days included in the post-emergency model: ", length(unique(res_mlVAR_post[["data"]][["BEEP"]]))),
      quote=FALSE)

#plot networks side-by-side
layout(t(1:2))
temp.pre<-plot(res_mlVAR_pre, "temporal", layout = "circle", nonsig = "hide", theme = "colorblind", 
               title = "temporal pre", 
               labels = shortnames, 
               label.scale.equal=TRUE, label.cex=2, 
               edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
               vsize = 15, asize = 10, mar=c(10,5,10,5))
temp.post<-plot(res_mlVAR_post, "temporal", layout = temp.pre$layout, nonsig = "hide", theme = "colorblind", 
               title = "temporal post", 
               labels = shortnames, 
               label.scale.equal=TRUE, label.cex=2, 
               edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
               vsize = 15, asize = 10, mar=c(10,5,10,5))

layout(t(1:2))
bw.pre<-plot(res_mlVAR_pre, "between", layout = temp.pre$layout, nonsig = "hide",  theme = "colorblind", 
               title = "between-subjects pre", 
               labels = shortnames, 
               label.scale.equal=TRUE, label.cex=2, 
               edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
               vsize = 15, rule = "and", asize = 10, mar=c(10,5,10,5))
bw.post<-plot(res_mlVAR_post, "between", layout = temp.pre$layout, nonsig = "hide",  theme = "colorblind", 
               title = "between-subjects post", 
               labels = shortnames, 
               label.scale.equal=TRUE, label.cex=2, 
               edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
               vsize = 15, rule = "and", asize = 10, mar=c(10,5,10,5))

layout(t(1:2))
plot(res_mlVAR_pre, "contemporaneous", layout = temp.pre$layout, nonsig = "hide", theme = "colorblind", 
     title = "contemporaneous pre", 
     labels = shortnames, 
     label.scale.equal=TRUE, label.cex=2, edge.labels=TRUE,
     vsize = 15, rule = "and", asize = 10, mar=c(5,5,10,5))
plot(res_mlVAR_post, "contemporaneous", layout = temp.pre$layout, nonsig = "hide", theme = "colorblind", 
     title = "contemporaneous post",
     labels = shortnames, 
     label.scale.equal=TRUE, label.cex=2, edge.labels=TRUE,
     vsize = 15, rule = "and", asize = 10, mar=c(5,5,10,5))

#summary of models and correlation /covariation matrices
print(paste("Pre-emergency model summary: "), quote=FALSE)
summary(res_mlVAR_pre)
print(paste("Pre-emergency correlation matrix: "), quote=FALSE)
res_mlVAR_pre$results$Omega_mu$cor$mean
print(paste("Post-emergency model summary: "), quote=FALSE)
summary(res_mlVAR_post)
print(paste("Post-emergency correlation matrix: "), quote=FALSE)
res_mlVAR_post$results$Omega_mu$cor$mean
```

```{r mlVAR_prepost_lag_compare}
#pre
res_mlVAR_pre.1<-mlVAR(dfB_pre, vars, idvar, lags = c(1), temporal="correlated", estimator="lmer",
                     compareToLags=c(1,2))
res_mlVAR_pre.12<-mlVAR(dfB_pre, vars, idvar, lags = c(1,2), temporal="correlated", estimator="lmer",
                     compareToLags=c(1,2))
mlVARcompare(res_mlVAR_pre.1, res_mlVAR_pre.12)

#post
res_mlVAR_post.1<-mlVAR(dfB_post, vars, idvar, lags = c(1), temporal="correlated", estimator="lmer",
                     compareToLags=c(1,2))
res_mlVAR_post.12<-mlVAR(dfB_post, vars, idvar, lags = c(1,2), temporal="correlated", estimator="lmer",
                     compareToLags=c(1,2))
mlVARcompare(res_mlVAR_post.1, res_mlVAR_post.12)
```

Let's next examine the stability of the estimated networks using the case-drop bootstrap approach described in (Epskamp, Borsboom, & Fried, 2017). 

Specifically, we will follow the multi-subject example outlined in Epskamp (2020) (see 10.1007/s11336-020-09697-3), i.e., estimate 1000 bootstrapped networks with a random 25% of participants dropped from the original sample each time.

```{r mlVAR_prepost_stability_bootstrap}
#make sure only sampling users from pre-post models
users<-as.numeric(res_mlVAR_pre[["IDs"]])

#create subdir to save boots for reproducibility purposes
dir.create('./network-2-bootstraps/pre/')
dir.create('./network-2-bootstraps/post/')

nBoots=1000
users.boot<-vector("list", length(nBoots))
for (bootNo in seq(1, nBoots)) {
  #select random sample of 75% of users (25% subject-level case drop)
  users.boot<-sample(users, round(0.75*length(users)))
  #grab data for these users
  dfB_pre.boot<-dfB_pre %>%
         filter(user %in% users.boot)
  dfB_post.boot<-dfB_post %>%
         filter(user %in% users.boot)
    tryCatch({
      #run mlVAR models on case-dropped subsamples [pre-lockdown]
      res_mlVAR_pre.boot<-mlVAR(dfB_pre.boot, vars, idvar, lags = c(1), 
                                temporal="correlated", estimator="lmer")
      saveRDS(res_mlVAR_pre.boot, file = paste0("./network-2-bootstraps/pre/mlVARboot", bootNo, ".rds"))
      #run mlVAR models on case-dropped subsamples [pre-lockdown]
      res_mlVAR_post.boot<-mlVAR(dfB_post.boot, vars, idvar, lags = c(1), 
                                temporal="correlated", estimator="lmer")
      saveRDS(res_mlVAR_post.boot, file = paste0("./network-2-bootstraps/post/mlVARboot", bootNo, ".rds"))
    }, error=function(e){})
}
```

Now, let's look at the results of the case-drop bootstrap for each network (code stolen from Epskamp 2020; https://osf.io/7b9ms/)

```{r mlVAR_prepost_stability_results}
#define some helper functions (adapted from those in Epskamp 2020 supplementary material, see: https://osf.io/9gq2d/):
edgeInclude <- function(x, mat, transpose = FALSE){
  n <- length(x)
  res <- 1/n * Reduce("+", lapply(x, function(x) x[[mat]]!=0))
  if (transpose){
    res <- t(res)
  }
  nBoots * res
}

signInclude <- function(x, mat, transpose = FALSE, lowertri = FALSE){
  Pos <- lapply(x, function(x) x[[mat]]>0)
  inclPos <- 1/length(x) * Reduce("+",Pos)
  dfPos <- data.frame(
    from = c(row(inclPos)),
    to = c(col(inclPos)),
    incl = c(inclPos),
    type = "pos"
  )
  Neg <- lapply(x, function(x) x[[mat]]<0)
  inclNeg <- 1/length(x) * Reduce("+",Neg)
  dfNeg <- data.frame(
    from = c(row(inclNeg)),
    to = c(col(inclNeg)),
    incl = c(inclNeg),
    type = "neg"
  )
  tab <- rbind(dfPos,dfNeg)
  if (transpose){
    tab[,1:2] <- tab[,2:1]
  }
  if (lowertri){
    tab <- tab[tab$from >= tab$to,]
  }
  tab
}

#####pre-lockdown###########
#read 75% bootstraps:
files<-list.files(path="./network-2-bootstraps/pre", pattern="mlVARboot", full.names=TRUE)
bootstraps<-vector("list", length(files))
for (f in seq (1, length(files))) {
  bootstraps[[f]]$temporal<-getNet(readRDS(files[f]), "temporal")
  bootstraps[[f]]$contemporaneous<-getNet(readRDS(files[f]), "contemporaneous")
  bootstraps[[f]]$between<-getNet(readRDS(files[f]), "between")
}
saveRDS(bootstraps, "./network-2-bootstraps/pre/all_bootstrapped_nets.rds")

#read full sample estimated network:
res_mlVAR_pre<-readRDS("./network-2-results/results_mlVAR_pre.rds")
temporal.pre<-getNet(res_mlVAR_pre, "temporal")
contemporaneous.pre<-getNet(res_mlVAR_pre, "contemporaneous")
between.pre<-getNet(res_mlVAR_pre, "between")

#get bootstrap results (number of times edges retained across bootstraps):
#inclusion over edges (disregard sign):
nBoots=length(files)
#temporal:
tempInclude_noSign<-edgeInclude(bootstraps, "temporal")
print(paste("Pre-emergency temporal network edge inclusion: "), quote=FALSE)
print(tempInclude_noSign)
#contemporaneous (lowertri);
contInclude_noSign<-edgeInclude(bootstraps, "contemporaneous")
print(paste("Pre-emergency contemporaneous network edge inclusion: "), quote=FALSE)
print(contInclude_noSign)
#between (lowertri):
betInclude_noSign<-edgeInclude(bootstraps, "between")
print(paste("Pre-emergency between-users network edge inclusion: "), quote=FALSE)
print(betInclude_noSign)

#visualise bootstrap inclusions probabilities
tempInclude<-signInclude(bootstraps, "temporal", transpose = FALSE)
contInclude<-signInclude(bootstraps, "contemporaneous", lowertri = TRUE)
betInclude<-signInclude(bootstraps, "between", lowertri = TRUE)

qgraph(tempInclude[,1:3], layout = temp.pre$layout, theme = "colorblind",  
       vsize = 15, asize = 10, mar=c(5,5,10,5), directed = TRUE,
       labels = shortnames,  maximum = 1, esize = 10,
       edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
       edge.color = ifelse(tempInclude$type=="pos","#0000D5","#BF0000"),
       vTrans = 254, diag = TRUE, label.scale.equal = TRUE, label.cex=2, parallelEdge = TRUE)

qgraph(contInclude[,1:3], layout = temp.pre$layout, theme = "colorblind", 
       vsize = 15, asize = 10, mar=c(5,5,10,5), directed = FALSE,
       labels = shortnames, maximum = 1, esize = 10,
       edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
       edge.color = ifelse(contInclude$type=="pos","#0000D5","#BF0000"),
       vTrans = 254, label.scale.equal = TRUE, label.cex=2, parallelEdge = TRUE)

qgraph(betInclude[,1:3], layout = temp.pre$layout, theme = "colorblind",
       vsize = 15, asize = 10, mar=c(5,5,10,5), directed = FALSE,
       labels = shortnames, maximum = 1, esize = 10,
       edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
       edge.color = ifelse(betInclude$type=="pos","#0000D5","#BF0000"),
       vTrans = 254, label.scale.equal = TRUE, label.cex=2, parallelEdge = TRUE)

#####post-lockdown###########
#read 75% bootstraps:
files<-list.files(path="./network-2-bootstraps/post", pattern="mlVARboot", full.names=TRUE)
bootstraps<-vector("list", length(files))
for (f in seq (1, length(files))) {
  bootstraps[[f]]$temporal<-getNet(readRDS(files[f]), "temporal")
  bootstraps[[f]]$contemporaneous<-getNet(readRDS(files[f]), "contemporaneous")
  bootstraps[[f]]$between<-getNet(readRDS(files[f]), "between")
}
saveRDS(bootstraps, "./network-2-bootstraps/post/all_bootstrapped_nets.rds")

#read full sample estimated network:
res_mlVAR_post<-readRDS("./network-2-results/results_mlVAR_post.rds")
temporal.post<-getNet(res_mlVAR_post, "temporal")
contemporaneous.post<-getNet(res_mlVAR_post, "contemporaneous")
between.post<-getNet(res_mlVAR_post, "between")

#get bootstrap results (number of times edges retained across bootstraps):
#inclusion over edges (disregard sign):
nBoots = length(files)
#temporal:
tempInclude_noSign<-edgeInclude(bootstraps, "temporal")
print(paste("Post-emergency temporal network edge inclusion: "), quote=FALSE)
print(tempInclude_noSign)
#contemporaneous (lowertri);
contInclude_noSign<-edgeInclude(bootstraps, "contemporaneous")
print(paste("Post-emergency contemporaneous network edge inclusion: "), quote=FALSE)
print(contInclude_noSign)
#between (lowertri):
betInclude_noSign<-edgeInclude(bootstraps, "between")
print(paste("Post-emergency between-users network edge inclusion: "), quote=FALSE)
print(betInclude_noSign)

#visualise bootstrap inclusions probabilities
tempInclude<-signInclude(bootstraps, "temporal", transpose = FALSE)
contInclude<-signInclude(bootstraps, "contemporaneous", lowertri = TRUE)
betInclude<-signInclude(bootstraps, "between", lowertri = TRUE)

qgraph(tempInclude[,1:3], layout = temp.pre$layout, theme = "colorblind",  
       vsize = 15, asize = 10, mar=c(5,5,10,5), directed = TRUE,
       labels = shortnames,  maximum = 1, esize = 10,
       edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
       edge.color = ifelse(tempInclude$type=="pos","#0000D5","#BF0000"),
       vTrans = 254, diag = TRUE, label.scale.equal = TRUE, label.cex=2, parallelEdge = TRUE)

qgraph(contInclude[,1:3], layout = temp.pre$layout, theme = "colorblind", 
       vsize = 15, asize = 10, mar=c(5,5,10,5), directed = FALSE,
       labels = shortnames, maximum = 1, esize = 10,
       edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
       edge.color = ifelse(contInclude$type=="pos","#0000D5","#BF0000"),
       vTrans = 254, label.scale.equal = TRUE, label.cex=2, parallelEdge = TRUE)

qgraph(betInclude[,1:3], layout = temp.pre$layout, theme = "colorblind",
       vsize = 15, asize = 10, mar=c(5,5,10,5), directed = FALSE,
       labels = shortnames, maximum = 1, esize = 10,
       edge.labels=TRUE, edge.label.cex=1.3, edge.label.margin=0.04,
       edge.color = ifelse(betInclude$type=="pos","#0000D5","#BF0000"),
       vTrans = 254, label.scale.equal = TRUE, label.cex=2, parallelEdge = TRUE)
```

Next, let's examine power-related properties of the estimated networks at different sample sizes using the simulation-based approach taken in Epskamp (2020). 

Specifically, we will investigate the sensitivity, specificity, and the correlation between original ('true') and estimated network parameters at each sample size using 100 simulated networks (see also Epskamp & Fried, 2018).

```{r mlVAR_prepost_power}
########pre-emergency#############
object<-res_mlVAR_pre
########pre-emergency#############
#object<-res_mlVAR_post

##################################
## Setup data to base simulation on:
# Get Beta matrices
Beta<-object$results$Beta$subject
# Check if all are stationary:
stationary <- sapply(Beta,function(b){
    ev <- eigen(b[,,1])$values
    all(Re(ev)^2 + Im(ev)^2 < 1)
})
# Inverse thetas:
invTheta <- lapply(object$results$Theta$cov$subject,solve)
# Check for outling variance in contemoraneous cov matrices:
contVar <- sapply(invTheta,function(x)sum(diag(solve(x))))
# Remove all 10* higher than median:
goodVar <- contVar < 10*median(contVar)
# Intercepts:
Means <- object$results$mu$subject
if (any(!goodVar | !stationary)){
    warning(paste0("Model for subject(s) ",paste(which(!goodVar | !stationary),collapse=" "),
                   " are not proper removed from simulation"))
    keep <- goodVar | stationary
    invTheta <- invTheta[keep]
    Beta <- Beta[keep]
    Means <- Means[keep]
}
  
### SIMULATION ###
nCores = 8
nTime = 36 #34 for post-emergency
nSubject = c(22,100,250,500)
nReps = 100
  
Results <- parSim(
    # timepoints conditions:
    nTime = nTime,
    # number of subjects conditions:
    nSubject = nSubject, #length(Beta)
    # Setup:
    name = "mlVARsim",
    write = FALSE,
    nCores = nCores, # Change this to use more or less computer cores
    reps = nReps, # Number of repetitions per condition
    debug = FALSE,
    export = c("cor0","CompareNetworks","bias","object","invTheta","Means","Beta"),
    
    # The simulation code:
    expression = {
      # Input
      input <- c(object$input)
      input$idvar <- "id"
      
      # Simulate data:
      simData <- mapply(id = 1:nSubject, b = Beta, k = invTheta, m = Means, SIMPLIFY = FALSE, 
                        FUN = function(id, b, k, m){
                          data <- graphicalVAR::graphicalVARsim(nTime, b[,,1], k, m)
                          data <- as.data.frame(data)
                          names(data) <- input$vars
                          data$id <- id
                          data
                        })
      simData <- do.call(rbind, simData)
      
      # Fit model:
      Res <- do.call(mlVAR::mlVAR, c(list(data=simData), input))
      
      #Fixed effectsthresholded:
      # True networks:
      truePDC <- mlVAR::getNet(object, "temporal", nonsig = "hide")
      truePCC <- mlVAR::getNet(object, "contemporaneous", nonsig = "hide", rule = "and")
      trueBW <- mlVAR::getNet(object, "between", nonsig = "hide", rule = "and")
      # Estimated networks:
      estPDC <- mlVAR::getNet(Res, "temporal", nonsig = "hide")
      estPCC <- mlVAR::getNet(Res, "contemporaneous", nonsig = "hide", rule = "and")
      estBW <-  mlVAR::getNet(Res, "between", nonsig = "hide", rule = "and")
      # Comapare est vs true:
      PDCres <- CompareNetworks(truePDC, estPDC, directed = TRUE)
      PDCres$network <- "temporal"
      PCCres <- CompareNetworks(truePCC, estPCC, directed = FALSE)
      PCCres$network <- "contemporaneous"
      BWres <- CompareNetworks(trueBW, estBW, directed = FALSE)
      BWres$network <- "between"
      
      Results <- data.frame(
        network = c("temporal_fixed", "contemporaneous_fixed", "between"),
        correlation = c(PDCres$Correlation, PCCres$Correlation, BWres$Correlation), 
        sensitivity = c(PDCres$sensitivity, PCCres$sensitivity, BWres$sensitivity),
        specificity = c(PDCres$specificity, PCCres$specificity, BWres$specificity),
        truePos = c(PDCres$TruePos, PCCres$TruePos, BWres$TruePos),
        falsePos = c(PDCres$FalsePos, PCCres$FalsePos, BWres$FalsePos),
        trueNeg = c(PDCres$TrueNeg, PCCres$TrueNeg, BWres$TrueNeg),
        falseNeg = c(PDCres$FalseNeg, PCCres$FalseNeg, BWres$FalseNeg)
      )
      return(Results)
    }) 
saveRDS(Results, file="./network-2-simulations/network-2-sim-Results-pre.rds")

#helper functions:
bias <- function(x,y) mean(abs(x-y), na.rm=TRUE)
CompareNetworks <- function(true,est, directed = TRUE){
  if (is.matrix(true) & is.matrix(est)){
    if (directed){
      real <- c(true)
      est <- c(est)
    } else {
      real <- true[upper.tri(true,diag=FALSE)]
      est <- est[upper.tri(est,diag=FALSE)]
    }        
  } else {
    real <- true
  }
  # True positives:
  TruePos <- sum(est != 0 &  real != 0)
  # False pos:
  FalsePos <- sum(est != 0 & real == 0)
  # True Neg:
  TrueNeg <- sum(est == 0 & real == 0)
  # False Neg:
  FalseNeg <- sum(est == 0 & real != 0)
  
  out <- list()
  # Sensitivity:
  out$sensitivity <- TruePos / (TruePos + FalseNeg)
  # Specificity:
  out$specificity <- TrueNeg / (TrueNeg + FalsePos)
  # Correlation:
  out$Correlation <- cor0(est, real)
  # Bias
  out$Bias <- bias(est,real)
  # rest
  out$TruePos <- TruePos
  out$FalsePos <- FalsePos
  out$TrueNeg <- TrueNeg
  out$FalseNeg <- FalseNeg
  
  return(out)
}
cor0 <- function(x,y,...){
  if (sum(!is.na(x)) < 2 || sum(!is.na(y)) < 2 || sd(x,na.rm=TRUE)==0 | sd(y,na.rm=TRUE) == 0){
    return(0)
  } else {
    return(cor(x,y,...))
  }
}
```

```{r mlvar_prepost_power_results, echo=FALSE}
########pre-emergency#############
sim.files<-list.files(pattern = "network-2-sim-Results-pre", path = "./network-2-simulations/", full.names = TRUE)
########post-emergency#############
# sim.files<-list.files(pattern = "network-2-sim-Results-post", path = "./network-2-simulations/", full.names = TRUE)

#gather data
sim.gathered <- sim.res %>% 
  dplyr::select(nSubject, network, sensitivity, specificity, correlation) %>%
  gather(metric, value, sensitivity:correlation)

#summarize
sim.summarized <- sim.gathered %>%
  group_by(network, nSubject, metric) %>%
  summarize(
    mean=mean(value, na.rm=TRUE),
    median=median(value, na.rm=TRUE),
    q25=quantile(value, 0.25, na.rm=TRUE),
    q75=quantile(value, 0.75, na.rm=TRUE)
  )
sim.temp <- sim.gathered %>%
  filter(network=="temporal")

#plot
plot.sim <- ggplot(sim.summarized, aes(x = factor(nSubject), y = mean, colour = network, group = network)) +
  geom_line() + geom_point() +
  facet_grid( ~ metric) + 
  theme_bw() + 
  xlab("Number of users")+
  ylab("") +
  scale_fill_discrete("") + 
  theme(legend.position = "top") + 
  ggplot2::scale_y_continuous(limits=c(0,1),breaks=seq(0,1,by=0.2)) +
  theme( panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  scale_colour_discrete("") + 
  guides(col = guide_legend(nrow = 3, byrow = FALSE), size = FALSE, alpha = FALSE) + 
  theme(legend.text=element_text(size=7)) + 
  scale_size_manual(values = c(1,1.5))
plot.sim

plot.sim.temp<-ggplot(sim.temp, aes(x = factor(nSubject), y = value)) +
  geom_boxplot(outlier.size = 0.5, lwd=0.5, fatten=0.5, position = position_dodge2(preserve = "total")) +
  facet_grid( ~ metric) + 
  theme_bw() +
  xlab("Number of users") + 
  ylab("") +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  theme(legend.position = "top")
plot.sim.temp  
```
